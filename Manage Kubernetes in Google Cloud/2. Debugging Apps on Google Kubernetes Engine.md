
## Overview

Cloud Logging, and its companion tool, Cloud Monitoring, are full featured products that are both deeply integrated into Google Kubernetes Engine. This lab teaches you how Cloud Logging works with GKE clusters and applications and some best practices for log collection through common logging use cases.


### The demo application used in the lab

To use a concrete example, you will troubleshoot a sample [microservices demo](https://github.com/GoogleCloudPlatform/microservices-demo) app deployed to a GKE cluster. In this demo app, there are many microservices and dependencies among them. You will generate traffic using a loadgenerator and then use Logging, Monitoring and GKE to notice the error (alert/metrics), identify a root cause with Logging and then fix/confirm the issue is fixed with Logging and Monitoring.

![Cloud Logging architecture diagram](https://cdn.qwiklabs.com/eqRuoUffBEQjVHqpX4Jw9i4CaLODvnZyOVd2JIOlpoA%3D)
## Task 2. Deploy application

Next, deploy a microservices application called Hipster Shop to your cluster to create a workload you can monitor.

1. Run the following to clone the repo:

git clone https://github.com/xiangshen-dk/microservices-demo.git

Copied!

content_copy

2. Change to the `microservices-demo` directory:

cd microservices-demo

Copied!

content_copy

3. Install the app using `kubectl`:

kubectl apply -f release/kubernetes-manifests.yaml

Copied!

content_copy

4. Confirm everything is running correctly:

kubectl get pods

Copied!

content_copy

The output should look similar to the output below:

NAME                                     READY     STATUS      RESTARTS     AGE
adservice-55f94cfd9c-4lvml               1/1       Running     0            20m
cartservice-6f4946f9b8-6wtff             1/1       Running     2            20m
checkoutservice-5688779d8c-l6crl         1/1       Running     0            20m
currencyservice-665d6f4569-b4sbm         1/1       Running     0            20m
emailservice-684c89bcb8-h48sq            1/1       Running     0            20m
frontend-67c8475b7d-vktsn                1/1       Running     0            20m
loadgenerator-6d646566db-p422w           1/1       Running     0            20m
paymentservice-858d89d64c-hmpkg          1/1       Running     0            20m
productcatalogservice-bcd85cb5-d6xp4     1/1       Running     0            20m
recommendationservice-685d7d6cd9-pxd9g   1/1       Running     0            20m
redis-cart-9b864d47f-c9xc6               1/1       Running     0            20m
shippingservice-5948f9fb5c-vndcp         1/1       Running     0            20m

5. Rerun the command until all pods are reporting a **Running** status before moving to the next step.


6. Run the following to get the **external IP** of the application. This command will only return an IP address once the service has been deployed, so you may need to repeat the command until there's an external IP address assigned:

export EXTERNAL_IP=$(kubectl get service frontend-external | awk 'BEGIN { cnt=0; } { cnt+=1; if (cnt > 1) print $4; }')

Copied!

content_copy

7. Finally, confirm that the app is up and running:

curl -o /dev/null -s -w "%{http_code}\n"  http://$EXTERNAL_IP



Your confirmation will look like this:

200

After the application is deployed, you can also go to the Cloud Console and view the status.

In the **Kubernetes Engine **>** Workloads** page you'll see that all of the pods are OK.

![The Workloads page](https://cdn.qwiklabs.com/dCTTlAg8y%2BMvkLYOfsHV79gWaP8BVe128PjUchzgUIY%3D)

8. Now click on **Gateways,Services & Ingress**, verify all services are OK. Stay on this screen to set up monitoring for the application.
## Task 3. Open the application

1. Scroll down to **frontend-external** and click the Endpoints IP of the service.

![The Services and Ingress page displaying the highlighted frontend-external IP address](https://cdn.qwiklabs.com/CnDr%2Fjy1XOpf%2BxtzxDsBgMhN4isBLGZYUkiIp2cbblc%3D)

It should open the application and you will have a page like the following:

![The Online Boutique web page displaying product tiles](https://cdn.qwiklabs.com/CJlTEQEyRcWnA3UPpBrAGeg7Q2WPwF8JroWIAmnYSw4%3D)

## Task 4. Create a logs-based metric

Now you will configure Cloud Logging to create a [logs-based metric](https://cloud.google.com/logging/docs/logs-based-metrics), which is a custom metric in Cloud Monitoring made from log entries. Logs-based metrics are good for counting the number of log entries and tracking the distribution of a value in your logs. In this case, you will use the logs-based metric to count the number of errors in your frontend service. You can then use the metric in both dashboards and alerting.

1. Return to the Cloud Console, and from the **Navigation menu** open **Logging**, then click **Logs Explorer**.

![The Logs Explorer page](https://cdn.qwiklabs.com/a6qaOKb29LCvCi8kqGa6M%2BaHSRmGV1vsihxTVXNsm8M%3D)

2. Enable **Show query** and in the **Query builder** box, add the following query:

resource.type="k8s_container"
severity=ERROR
labels."k8s-pod/app": "recommendationservice"

Copied!

content_copy

![The Query builder page displaying the three lines in the query above](https://cdn.qwiklabs.com/NEhcKxwC7scWkseAC5U27OujggdZJApIa7Im9Vq9kd4%3D)

3. Click **Run Query**.

The query you are using lets you find all errors from the frontend pod. However, you shouldn't see any results now since there are no errors yet.

4. To create the logs-based metric, click on **Create Metric**.

![The Create metric button displayed on the UI](https://cdn.qwiklabs.com/b80Z%2B2QNOiA130inlQNO51SRTqwpSqPc1hFE6s4%2Bnxw%3D)

5. Name the metric **Error_Rate_SLI,** and click **Create Metric** to save the log based metric:

![The Create logs metric dialog displaying the populated Log metric name field](https://cdn.qwiklabs.com/4mIaP%2BVvRIyYECQypwhKk8suBFyTQVqX6a92ZIUXQUQ%3D)

You now see the metric listed under User-defined Metrics on the Logs-based Metrics page

## Task 5. Create an alerting policy

Alerting gives timely awareness to problems in your cloud applications so you can resolve the problems quickly. Now you will use Cloud Monitoring to monitor your frontend service availability by creating an alerting policy based on the frontend errors logs-based metric that you created previously. When the condition of the alerting policy is met, Cloud Monitoring creates and displays an incident in the Cloud console.

1. In the **Navigation menu**, open **Monitoring,** then click **Alerting**.
    
2. After the workspace is created, click **Create Policy** at the top.
    

**Note:** If required, click **Try It!** to use the updated alert creation flow.

3. Click on **Select a metric** dropdown. Unselect the **Active**
    
4. In **filter by resource and metric name** field, type **Error_Rate**.
    
5. Click on **Kubernetes Container > Logs-Based Metric**. Select **logging/user/Error_Rate_SLI** and click **Apply**.
    

Your screen should look like this:

![The Select a metric page](https://cdn.qwiklabs.com/9BLbh6RWDSQboUEZMptmizL5wSQAcMJtApW%2FD7R6BH4%3D)

6. Set **Rolling windows function** to `Rate`.
    
7. Click **Next**.
    
8. Set **0.5** as your **Threshold value**.
    

As expected, there are no failures, and your application is meeting its availability Service Level Objective (SLO).

9. Click **Next** again.
    
10. Disable **Use notification channel**.
    
11. Provide an alert name such as `Error Rate SLI` then click **Next**.
    
12. Review the alert and click **Create Policy**.
    

**Note:** You will not create a notification channel for this lab but you should do it for your applications running in production, which allows you to send notifications in ways such as email, mobile app, SMS, Pub/Sub, and webhooks.

### **Trigger an application error**

Now you will use a load generator to create some traffic for your web application. Since there is a bug that has been intentionally introduced into this version of the application, a certain amount of traffic volume will trigger errors. You will work through the steps to identify and fix the bug.

1. From the **Navigation menu**, select **Kubernetes Engine**, then **Gateways,Services & Ingress**.
    
2. Find the `loadgenerator-external` service, then click on the `endpoints` link.
    

![The Services and Ingress page open on the Services tabbed page, which displays the highlighted loadgenerator-external service and endpoints link.](https://cdn.qwiklabs.com/b7Ac1Avr%2F4EiNLfm6yXwhk8qSDgg3cHgicPsXYVmXfU%3D)

Alternatively, you can open a new browser tab or window, copy/paste the IP to the URL field, for example: `http://\[loadgenerator-external-ip\]`

You should now be on the Locust load generator page:

![The Locust load generator page](https://cdn.qwiklabs.com/KYnJGu3AM6b1gbzam%2BA2oQx4pJLFGLdhxoGVWiF1YzM%3D)

Locust is an open-source load generator, which allows you to load test a web app. It can simulate a number of users simultaneously hitting your application endpoints at a certain rate.

1. Simulate **300** users hitting the app with a hatch rate of **30**. Locust will add 30 users per second until it reaches 300 users.
    
2. For the host field, you will use the `frontend-external`. Copy the URL from the Gateways,Services & Ingress page; be sure to exclude the port. For example:
    

![The Start new Locust swarm page displaying the Start swarming button](https://cdn.qwiklabs.com/VfHVzfR14oTKeil2PkbOl0ozSJqcFfvJWp%2BqRSLVhhA%3D)

3. Click the **Start swarming** button. You should have about 300 users to hit the predefined URLs in a few seconds.

![The Statistics page displaying the list of 300 users](https://cdn.qwiklabs.com/Ncjbt1KMUPopuYiMIT%2BJWqpMSY%2BNXhvhwuw2vNCKks8%3D)

4. Click on the **Failures** tab to see that there are failures starting to occur. You can see there are a large number of 500 errors.

![The Failures tabbed page](https://cdn.qwiklabs.com/fxsEwlryECSVQKjvHeugf%2FXpxz5sy3TvMI02PEi1q4o%3D)

Meanwhile, if you click any product from the home page, it's either noticeably slow or you receive errors like the following if you click on a product:

![The Online Boutique displaying the HTTP Status error: 500 internal server error.](https://cdn.qwiklabs.com/ZdTffxzZfDLeT8y7VMRpa2i5QuXq8MHir6ShIEKAWcU%3D)

### **Confirming the alert and application errors**

1. In the console, from the **Navigation menu**, click **Monitoring**, then **Alerting**. You should see an incident soon regarding **logging/user/Error_Rate_SLI**. If you don't see an incident right away, wait a minute or two and refresh your page. It can take up to 5 minutes for the alert to fire.
    
2. Click the link of the incident:
    

![The Alerting page displaying the incident link in the Incidents section](https://cdn.qwiklabs.com/2zBtciU9BcGHRXLgmxGN2hx4BE6crILcBmgpdcOaKHs%3D)

It brings you to the details page.

3. Click the **VIEW LOGS** link to view the logs for the pod.

![The Incident metrics page displaying the highlighted View logs button](https://cdn.qwiklabs.com/5%2BGP06ov45iZoTHJ%2FBSqDFQhwvKzXE%2FyozK1gOI5xOk%3D)

4. You can also click the **Error** label in the Logs field explorer panel to only query the errors.

Alternatively, you can click into the Query preview field to show the query builder, then click the **Severity** dropdown, add **Error** to the query. Click the **Add** button, then click **Run Query**. The dropdown menu allows adding multiple severity values.

The result either way is adding `severity=ERROR` to your query. Once you do that, you should have all the errors for the recommendationservice pod.

![The Logs Explorer page open on the Query builder tabbed page, displaying a list of errors in the Query results section](https://cdn.qwiklabs.com/78xP%2F4qjlF9PylIdfqDRtrV7Z54FNyyLG%2FvQFDjPM3U%3D)

5. View the error details by expanding an error event. For example:

![The expanded Connect Failed query result](https://cdn.qwiklabs.com/DxQeZywdRBJmyzzxrsy2pQz1CJh4s7y2Ss2TjoWY0yU%3D)

6. Expand the `textPayload`.
    
7. Click the error message and select **Add field to summary line** to have the error messages appearing as a summary field:
    

![The Add field to summary line option hihglighted in the expanded error message menu](https://cdn.qwiklabs.com/R3HpT921ecz3rjGH7bL3gjI8dKOArSfyXe2bBafBLF0%3D)

From there, you can confirm there are indeed many errors for the `RecommendationService` service. Based on the error messages, it appears the `RecommendationService` couldn't connect to some downstream services to either get products or recommendations. However, it's still not clear what the root cause is for the errors.

If you revisit the architecture diagram, the **RecommendationService** provides a list of recommendations to the **Frontend** services. However, both the **Frontend** service and the **RecommendationService** invoke **ProductCatalogService** for a list of products.

![The architecture diagram with the highlighted ProductCatalogService and RecomendationService categories.](https://cdn.qwiklabs.com/tMNwCX5zWS3uk4DX%2FKjuwIgNv8xO36qKc3bteUCvviQ%3D)

For the next step, you will look at the metrics of the main suspect, the **ProductCatalogService**, for any anomalies. Regardless, you can drill down in the logs to get some insights.

### **Troubleshooting using the Kubernetes dashboard & logs**

1. One of the first places that you can look at the metrics is the [Kubernetes Engine](https://console.cloud.google.com/monitoring/dashboards/resourceList/kubernetes) section of the Monitoring console (**Navigation menu** > **Monitoring**> **Dashboards** > **GKE**).
    
2. View the **Workloads** section.
    
3. Navigate to the **Kubernetes Engine** > **Workloads** > **productcatalogservice**. You can see the pod for the service is constantly crashing and restarting.
    

![The Active Revisions section highlighted on the Deployment details page](https://cdn.qwiklabs.com/1FH92uxx6Bb%2F5CcyHkE%2BaDfSDdZJzVz2atwRJSl9%2BfQ%3D)

Next, see if there is anything interesting in the logs.

There are 2 ways to easily get to your container logs:

1. Click on the **Logs** tab to get a quick view of the most recent logs. Next, click the external link button in the upper right corner of the logs panel to go back to the Logs Explorer.

![The Logs tabbed page](https://cdn.qwiklabs.com/tL8BVS9l1hsesA4XdULcpzbqw1iFcc%2FbXgqa2d3x8WU%3D)

2. In the overview page, click the **Container logs** link on the Deployment Details page.

![The Container logs link highlighted on the Deployment Details page](https://cdn.qwiklabs.com/%2FWe58kC8N5PpaG9uCIL5NUacTNMBe6bnZ5dd6F%2FA6LU%3D)

You are on the Logs Explorer page again, now with a predefined query specifically filtered for the logs from the container you were viewing in GKE.

From the Log Viewer, both the log messages and the histogram show the container is repeatedly parsing product catalogs within a short period of time. It seems very inefficient.

At the bottom of the query results, there might also be a runtime error like the following one:

panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation

This could actually be causing the pod to crash.

To better understand the reason, search the log message in the code.

1. In Cloud Shell, run the following command:

grep -nri 'successfully parsed product catalog json' src

Copied!

content_copy

Your output should look like the following, which has the source file name with a line number:

src/productcatalogservice/server.go:237:        log.Info("successfully parsed product catalog json")

2. To view the source file, by clicking the **Open Editor** button in the Cloud Shell menu, then **Open in New Window** (if you see the Unable to load code editor because third-party cookies are disabled error, click the eye at the top of the Chrome page).

![The Open Editor button highlighted in the UI](https://cdn.qwiklabs.com/FSDqao0b0m16GSMgPkmvkhS%2FNTcCfljPx5XojXOUy1c%3D)

3. Click the file `microservices-demo/src/productcatalogservice/server.go`, scroll down to line 237, and you will find the **readCatalogFile** method logs this message:

![The message: log.Info("successfully parsed product catalog json") return nil](https://cdn.qwiklabs.com/PyVxVoCEbE%2FHZSCWsvXg2CC8SC24ZDrJjOxzC%2Fc%2B%2F4Y%3D)

With a little more effort, you can see that if the boolean variable **reloadCatalog** is true, the service reloads and parses the product catalog each time it's invoked, which seems unnecessary.

If you search the **reloadCatalog** variable in the code, you can see it's controlled by the environment variable `ENABLE_RELOAD` and writes a log message for its state.

![The log message for the reloadCatalog state](https://cdn.qwiklabs.com/qy2ap9hP7UlWCISKeIZKChXQPToqad7EljCe%2Bww4I5Q%3D)

Check the logs again by adding this message to your query and determine if there are any entries that exist.

1. Return to the tab where Logs Explorer is open and add the following line to the query:

jsonPayload.message:"catalog reloading"

Copied!

content_copy

So the full query in your query builder is:

resource.type="k8s_container"
resource.labels.location="ZONE"
resource.labels.cluster_name="central"
resource.labels.namespace_name="default"
labels.k8s-pod/app="productcatalogservice"
jsonPayload.message:"catalog reloading"

Copied!

content_copy

2. Click **Run Query** again and find an "Enable catalog reloading" message in the container log. This confirms that the catalog reloading feature is enabled.

![The Enable catalog reloading message in the container log](https://cdn.qwiklabs.com/W3h5MtUjmHcs9cqYfuEhhINLs9ZpP%2FiGhUDRsjV%2FpPI%3D)

At this point you can be certain the frontend error is caused by the overhead to load the catalog for every request. When you increased the load, the overhead caused the service to fail and generate the error.

## Task 6. Fix the issue and verify the result

Based on the code and what you're seeing in the logs, you can try to fix the issue by disabling catalog reloading. Now you will remove the `ENABLE_RELOAD` environment variable for the product catalog service. Once you make the variable changes, then you can redeploy the application and verify that the changes have addressed the observed issue.

1. Click the **Open Terminal** button to return to the Cloud Shell terminal if it has closed.
    
2. Run the following command:
    

grep -A1 -ni ENABLE_RELOAD release/kubernetes-manifests.yaml

Copied!

content_copy

The output will show the line number of the environment variable in the manifest file:

373:        - name: ENABLE_RELOAD
374-          value: "1"

3. Delete those two lines to disable the reloading by running:

sed -i -e '373,374d' release/kubernetes-manifests.yaml

Copied!

content_copy

4. Then reapply the manifest file:

kubectl apply -f release/kubernetes-manifests.yaml

Copied!

content_copy

You will notice only the **productcatalogservice** is configured. The other services are unchanged.

5. Return to the Deployment detail page (**Navigation menu** > **Kubernetes Engine** > **Workloads** > **productcatalogservice**), and wait until the pod runs successfully. Wait 2-3 minutes or until you can confirm it stops crashing.

![The Deployment details page displaying the highlighted Active revisions section](https://cdn.qwiklabs.com/5P5TpF%2F%2FAca%2Fy94llwqx4C0vpZ3ODONgIYB2%2BPxKZqI%3D)

6. If you click the **Container logs** link again, you will see the repeating `successfully parsing the catalog json` messages are gone:

![The Query builder page](https://cdn.qwiklabs.com/X0NqVMkLTOyNy1hYHc7RnXJelXwPDI%2FKkDX7TmVSSNY%3D)

7. If you go back to the webapp URL and click the products on the home page, it's also much more responsive and you shouldn't encounter any HTTP errors.
    
8. Go back to the load generator, click the **Reset Stats** button in the top right. The failure percentage is reset and you should not see it increasing anymore.
    

![The failure percentage displaying 0 percent](https://cdn.qwiklabs.com/BGsaWaM%2BnKrfmTi2SqszNDlvOrWhXjLOfXcVM0hRfBQ%3D)

All above checks indicate that the issue is fixed. If you are still seeing the 500 error, wait another couple of minutes and try clicking on a product again.